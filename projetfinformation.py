# -*- coding: utf-8 -*-
"""ProjetFinFormation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1joSim6bMNVt0CERoVOuI1vtXBymrge_R

# **DESCRIPTION**

Ce projet vise à développer un système d'analyse des sentiments à partir des publications sur les réseaux sociaux. Grâce à des techniques de traitement du langage naturel, le système vise à identifier et catégoriser automatiquement les sentiments exprimés dans le contenu des réseaux sociaux.

# **ETAPE 1: COLLECTE ET EXPLORATION DES DONNEES**

# Chargement des données
"""

#Importation des bibliothèques
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

#Chargement des données
df = pd.read_csv('/content/sentiment_analysis.csv')

"""# **Exploration des données**"""

# Afficher les informations sur l'ensemble de données
df_info = df.info()

"""Il n'y a pas de valeur null dans notre jeu de données"""

#Affichage des données statistiques
df_description = df.describe()
df_description

#Affichage du contenu du fichier
df.head()

"""# **Prétraitement des données**

Identification de valeurs abérantes
"""

#Fonction d'identification de valeurs abérantes
def val_aberante(df, liste):
  aberante_vel = {}
  for column in liste:
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    thrshold = 1.5
    borneinf = Q1 - (thrshold * IQR)
    bornesup = Q3 + (thrshold * IQR)
    outeliers = df[(df[column] < borneinf) | (df[column] > bornesup)]
    aberante_vel[column] = len(outeliers)
  return  aberante_vel

liste = ['Year', 'Month', 'Day']
aberante  = val_aberante(df, liste)

print(aberante)

"""Nous constatons que nous avons des valeurs abérantes au niveau de la colonne Année. Nous allons procéder au traitement de ces valeurs abérantes"""

#Définiton de la position des valeurs abérante
def getoutliers(colonne):
  Q1 = df[colonne].quantile(0.25)
  Q3 = df[colonne].quantile(0.75)
  IQR = Q3 - Q1

  threshold = 1.5
  retour  = df[(df[colonne] < (Q1 - threshold * IQR)) | (df[colonne] > (Q3 + threshold * IQR))]
  return retour

outliers_year     = getoutliers('Year')

# traitement des valeurs aberrantes
#Remplacement des valeurs abérantes par la médiane
yaer_med = df['Year'].median()
df['Year'] = df['Year'].astype(float)
df.loc[df.index.isin(outliers_year.index), 'Year'] = yaer_med

"""# **Encodage des données catégorielles**"""

# Supprimer tous les espaces dans la colonne 'Nom'
df['Platform'] = df['Platform'].str.replace(' ', '', regex=False)

#Identification des valeurs uniques des colonnes de type catégoriel
df['Platform'].unique()

sentiment_code = {'positive' : 0, 'neutral' : 1, 'negative' : 2}
df['sentiment_encode'] = df['sentiment'].map(sentiment_code)

time_encode = {'morning': 0, 'noon': 1, 'evening': 2, 'night': 3}
df['time_encode'] = df['Time of Tweet'].map(time_encode)
platform_encode = {'Twitter' : 0, 'Facebook' :1, 'Instagram' : 2}
df['platform_encode'] = df['Platform'].map(platform_encode)

"""# **ETAPE 2 : VISUALISATION DES DONNEES**"""

#Importation des bibliothèques de visualisation
import plotly.express as px
import matplotlib.pyplot as plt

#Création d'une colonne anneemois
df['mois'] =   df['Month'].apply(lambda x: "{:02}".format(x))
df['annee'] = df['Year'].astype(int)
df['annee'] = df['annee'].astype(str)
df['annemois'] =  df['annee'] +''+ df['mois']
#df['annemois'].head()

#Dispersion des sentiment par plateforme

fig = px.scatter(
    df,
    x= "annemois",
    y ="sentiment",
    color = "Platform",
    title = "Dispersion des sentiments par plateforme",
    labels={
        "annemois": "Mois",
        "sentiment": "Sentiment",
        "Platform": "Plateforme"
    }
)

#Affichage de la visualisation
fig.show()

#Evolution des tweet au cours de la journéee par plateforme
gpe_data =  df.groupby(['time_encode', 'Platform', 'sentiment']).size().reset_index(name='count')
#Création de graphe à aires empilées
fig = px.area(
    gpe_data,
    x='time_encode',
    y='count',
    color='Platform',
    title='Lien entre l\'heure du tweet et le sentiment',
    labels={
        'time_encode': 'Heure du tweet',
        'count': 'Nombre de sentiments',
        'Platform': 'Plateforme'
    }
)

#Affichage de la visualisation
fig.show()

"""Nous constatons que la fréquence des tweets diffère d'une plateforme à l'autre tout le long de la journée.

Nous observons un nombre élévé de tweeks les matins sur facebook; ce nombre décroit progressivement jusqu'à mi journée; passant de 32 à 2 avant de remonter à 27 jusqu'en fin de journée.

Le nombre de tweets sur twitter et Instagram est presque constant du debut de journée (environ 12) jusq'en mi journée.
Le nombre de tweeks sur ces plateformes grimpe considérablement dans les mêmes proportions après la mi journée passant en moyenne de 12 à 24.

Prendant que le nombre de tweek reste quasi constant sur Instagram jusqu'en fin de journée (19), celui sur Twitter connait une chute considérable de la mi journée à la fin de la journée passant de 25 à 3.
"""

#Diagramme circulaire matérialisant la distribution des sentiments
fig = px.pie(df, names='sentiment', title='Distribution des sentiments')
fig.show()

"""Nous avons 33,2% des tweeks qui ont un sentiments positif; 39,9% de tweets qui ont un sentiment neutre et 26,9% de tweet à sentiment négatif"""

#Type de sentiment en fonction des période de la journée
#Diagramme à bar

fig = px.bar(
    df,
    x='Time of Tweet',
    y='sentiment',
    color ='sentiment',
    title='Portion de sentiments par périodes de la journée',
    labels={
        'Time of Tweet': 'heure du tweek',
        'sentiment': 'Sentiments',
    }
)

#Affichage de la visualisation
fig.show()

"""A l'analyse du diagramme, nous pouvons dire que les sentiment ne sont pas fonction des période de la journée.
En effet, quelque soit la période de la journée les sentiments négatifs sont supérieur au sentiment positifs.

# **CHOIX DU MODELE**

Nous avons trois valeur possible dans notre variable cible : Positif, neutre, négaril.

Les variables de sortie étant de type catégorielle, nous pensons que la classification est le modèle appropriè pour l'apprentissage.

Nous allons utiliser l'algorithme K-NEAEST-NEIGHBOR (KNN) pour entrainer notre modèle.
"""

#Choix des variables cibles et caractéristiques
x = df[['Year', 'Month', 'Day', 'time_encode', 'platform_encode']]
y = df['sentiment_encode']

"""# **PHASE 3 : ENTRAINEMENT DU MODELE**"""

#Importer les bibliothèques
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

#Entrainement du modèle
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=46)

knn=KNeighborsClassifier(n_neighbors=5)
knn.fit(x_train,y_train)

#préiction du modèle
y_pred=knn.predict(x_test)

#Evaluation du modèle
print('Précision = ',accuracy_score(y_pred,y_test))

"""# Optimisation du modèle"""

#Détermination de la bonne valeur de K
tentative=100
scores=[]
for k in range(1,100) :
    knn=KNeighborsClassifier(tentative-k)
    knn.fit(x_train,y_train)
    y_pred=knn.predict(x_test)
    print('Précision pour k =',k,'est :',round(accuracy_score(y_pred,y_test),2))
    scores.append(round(accuracy_score(y_pred,y_test),2))

"""La meilleure valeur de k est 25 dont la précision est 0.55"""

#Entrainement du modèle avec k = 25
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=46)

knn=KNeighborsClassifier(n_neighbors=25)
knn.fit(x_train,y_train)